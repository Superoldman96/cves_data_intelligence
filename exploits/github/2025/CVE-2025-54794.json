[
    {
        "cve": "CVE-2025-54794",
        "poc_github": true,
        "full_name": "AdityaBhatt3010/CVE-2025-54794-Hijacking-Claude-AI-with-a-Prompt-Injection-The-Jailbreak-That-Talked-Back",
        "owner_login": "AdityaBhatt3010",
        "owner_id": 96762636,
        "owner_html_url": "https://github.com/AdityaBhatt3010",
        "owner_avatar_url": "https://avatars.githubusercontent.com/u/96762636?v=4",
        "html_url": "https://github.com/AdityaBhatt3010/CVE-2025-54794-Hijacking-Claude-AI-with-a-Prompt-Injection-The-Jailbreak-That-Talked-Back",
        "description": "A high-severity prompt injection flaw in Claude AI proves that even the smartest language models can be turned into weapons \u2014 all with a few lines of code.",
        "fork": false,
        "created_at": "2025-08-06T08:29:35Z",
        "updated_at": "2025-10-28T14:26:19Z",
        "pushed_at": "2025-08-06T08:43:29Z",
        "stargazers_count": 5,
        "watchers_count": 5,
        "has_discussions": false,
        "forks_count": 0,
        "allow_forking": true,
        "is_template": false,
        "web_commit_signoff_required": false,
        "topics": "[]",
        "visibility": "public",
        "forks": 0,
        "watchers": 5,
        "score": 0,
        "subscribers_count": 0
    }
]